name: Crawl GitHub Stars (100k)

on:
  workflow_dispatch:
  push:
    branches: [ main ]

jobs:
  crawl:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16-alpine
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: postgres-user-name
          POSTGRES_PASSWORD: postgres-password
          POSTGRES_DB: ghcrawler
        options: >-
          --health-cmd="pg_isready -U postgres-user-name -d ghcrawler"
          --health-interval=5s
          --health-timeout=5s
          --health-retries=10

    env:
      DATABASE_URL: postgresql://postgres-user-name:postgres-password@localhost:5432/ghcrawler
      PYTHONPATH: src

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Wait for Postgres
        run: |
          until pg_isready -h localhost -p 5432 -U postgres-user-name; do
            echo "Waiting for Postgres..."; sleep 2;
          done

      - name: Init DB schema
        run: python scripts/init_db.py

      - name: Crawl 100k repos
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python -m ghcrawler.crawl_stars --target 100000

      - name: Export CSV
        run: python -m ghcrawler.crawl_stars --export csv --out data/export.csv

      - name: Upload CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: stars-export
          path: data/export.csv
